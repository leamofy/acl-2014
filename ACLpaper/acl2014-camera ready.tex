\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{helvet}
\usepackage{courier}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{relsize}
\usepackage{hhline}
\usepackage[usenames]{color}
\usepackage{colortbl,booktabs}
%\usepackage{slashbox}
\usepackage{balance}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{url}
\usepackage{algpseudocode}
\definecolor{mygray}{gray}{.5}

\renewcommand{\baselinestretch}{0.95}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


% Alternate titles.

\title{Are Two Heads Better than One? Crowdsourced Translation via a \\Two-Step Collaboration of Non-Professional Translators and Editors}

\author{Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch \\
Computer and Information Science Department, \\
University of Pennsylvania, Philadelphia, PA, U.S.A.\\
\normalsize{{\tt \{ruiyan,gmingkun,epavlick\}@seas.upenn.edu, ccb@cis.upenn.edu}}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Crowdsourcing is a viable mechanism for creating training data for machine translation.  It provides a low cost, fast turn-around way of processing large volumes of data. However, when compared professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is a necessity for crowdsourcing to work well. In this paper, we use a two-step collaboration process with translation and post-editing by non-professionals.  We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals. With appropriate quality control, we are able to distinguish acceptable translations from bad ones.  We demonstrate our model on the  Mechanical Turk translations of the NIST 2009 Urdu-to-English evaluation set, and show that our models are able to achieve better quality than previously published methods.
\end{abstract}

\section{Introduction}

Statistical machine translation (SMT) systems are trained using bilingual sentence-aligned parallel corpora.  Theoretically, SMT can be applied to any language pair, but in practice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, English-Chinese, French-English, etc.  SMT gets stuck in a severe bottleneck for many minority or `low resource' languages with insufficient data.
This drastically limits what languages SMT can be successfully applied to.
Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs.  Past approaches have examined harvesting translated documents from the web \cite{a10,a13,smith-EtAl:2013:ACL}, or discovering parallel fragments from comparable corpora \cite{a11,abdulrauf-schwenk:2009:EACL,a12}.
Until relatively recently, relatively little consideration has been given to creating parallel data from scratch.  This is because the cost of hiring professional translators is prohibitively high.  For instance, \newcite{a25} hoped to hire professional translators to create a relatively modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment.

Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators.  Facebook localized its web site into different languages using volunteers \cite{Facebook-Crowdsourced-Translation}.  DuoLingo turns translation into an educational game, and translates web content using its language learners \cite{vonAhn:2013:DLL:2449396.2449398}.
%Crowdsourcing translation research in the NLP community has focused on hiring workers on the Amazon Mechanical Turk platform   \cite{Callison-Burch2009}, rather than relying on volunteers or gamification.
Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk platform   \cite{Callison-Burch2009}.

 conducted a preliminary study that compared the quality of professional translators to non-professional translators hired on Amazon Mechanical Turk (MTurk) for translations from Spanish, German, French, Chinese into Urdu.  \newcite{zaidan-callisonburch:2011:ACL-HLT2011a}

More recent efforts have used crowdsourcing to create

{\sc todo - cite past work on crowdsourcing translation \nocite{*}}


A worthy effort would be seeking for higher quality translations at a lower cost. We notice that Amazon's Mechanical Turk (MTurk) provides a labor force platform at a cheap price for labor units, and hence we are able to hire a large group of translators, non-professional or even professional, at a similarly tempting low price. In the way of crowdsourcing, we could manage to create a large parallel corpus at a fraction of the cost of professional translators. With affordable human computation achieved, we aim at soliciting high quality translations out of the redundant, perhaps low-quality, disfluent generations. We have proposed a random walk of mutual reinforced ranking model based on a heterogenous graph consisting of translations and the workers on MTurk, named MTurkers. In particularly, we design a schema of collaborative crowdsourcing: translation followed by post-editing. We also include linguistic quality scoring as the ranking prior during the selection process. After these two steps, the collaboration of two non-professional generate a translated sentence. The model discriminates acceptable translations from those that are not (and analogously competent Turkers from those who are not).

According to an objective and quantitative comparison with the professionally-produced reference translations as a weights set, we examine the idea of enabling the 2-step of ``\textit{collaborations}'' between both roles of non-professional translators and post-editors, and using structural information from the collaboration relationships. We show that it is possible to get near professional high quality translations in aggregate by soliciting multiple translations to select the best of the bunch (and the cost can be even lower to identifying competent MTurkers). The overall performance is improved by the 2-step collaboration and the information hidden in the net structure: in other words, \textbf{TWO} heads are indeed better than \textbf{ONE}.


We start by introducing the crowdsourcing platform and data collection. In Section 3 we formulate a reinforced ranking model combined with random walk, and then describe experimental results in Section 4. We review related work in Section 5 and draw conclusions in Section 6.
%
%
%\section{Crowdsourcing Platform and Data Collection}
%To collect crowdsourced translations, we deploy our platform based on Amazon's Mechanical Turk, an online market-place designed to pay people small sums of money to complete \textit{Human Intelligence Tasks} (or HIT) – tasks that are difficult for computers but easy for people. Example HIT ranges from labeling images or annotating texts and semantics to providing feedback on relevance of results for search queries \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. Anyone with an Amazon account can either submit HIT or work on HIT that were submitted by others. Workers are referred to as ``Turkers'', and designers of HIT as ``Requesters''. A Requester specifies the reward to be paid for each completed item. The relationship between Turkers and Requesters is designed to be a mutual selection: Turkers are free to select whichever HIT interests them, and requesters can choose not to pay for unsatisfied results.
%
%The advantages of Mechanical Turk are obvious: zero overhead for hiring workers with a large, low-cost labor force and the task can be completed in a naturally parallel pattern by vast individuals so that the turnaround time is short. For Natural Language Processing, it is easier to access to foreign markets with native speakers of many rare languages. On the other hands, the Turkers are completely anonymous without any personal profile other than a Turker ID (eg., A143AWKU99STC9). Hence it is difficult to determine if a non-professional is qualified to fulfill the task before the Turker submits HIT.
%
%In this sense, soliciting translations from anonymous non-professionals carries a significant risk of poor translation quality, but it is not very difficult to find bad translations provided by Turkers using simple linear regression methods \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. To improve the accuracy of noisy translations from non-experts, a natural quality control solution would be employing a graph-based ranking model to quantitatively measure multiple outputs based on ``votes'' or ``recommendations'' between each other, namely the wisdom of crowds.
%

\section{Crowdsourcing Translation}
\subsection{Setup}
Our HIT involves showing the worker a sequence of sentences in source language (i.e., \textit{Urdu} in this work), and asking them to provide an English translation for each one. The screen also included a brief set of instructions, and a short questionnaire section. The reward was set at \$0.10 per translation, or roughly \$0.005 per word. We solicit three translations per Urdu sentence (from three distinct translators). we instead split the data set into groups of 10 sentences per HIT. We keep some of the strategies used in other crowdsourcing systems in designing interfaces. For instance, we converted the Urdu sentences into images so that Turkers cannot cheat by copying-and-pasting the Urdu text into an online commercial MT system such as Google translation.

%\subsection{One-Step Model}
%\subsection{Two-Step Collaboration Model}

\begin{table}[t]
\centering
\begin{tabular}{|l|} \hline
Urdu translator:\\ \hline
 \begin{minipage}[position]{0.45\textwidth}
  \vspace{1mm}\small{According to the territory's people the pamphlets from the Taaliban had been read in the announcements in all the mosques of the Northern Wazeerastan.}\vspace{1mm}
 \end{minipage}
\\ \hline \end{tabular}
\begin{tabular}{|l|} \hline
English post-editor:\\ \hline
 \begin{minipage}[position]{0.45\textwidth}
    \vspace{1mm}\small{According to locals, the pamphlet released by the Taliban was read out on the loudspeakers of all the mosques in North Waziristan.}\vspace{1mm}
 \end{minipage}
\\ \hline \end{tabular}
\begin{tabular}{|l|} \hline
LDC professional:\\ \hline
 \begin{minipage}[position]{0.45\textwidth}
      \vspace{1mm}\small{According to the local people, the Taliban's pamphlet was read over the loudspeakers of all mosques in North Waziristan.} \vspace{1mm}
 \end{minipage}
\\ \hline \end{tabular}
\caption{Different versions of translations.}
\label{translations}
\end{table}

%\begin{table}[t]
%\centering
%\begin{tabular}{|p{\linewidth}|} \hline
%Urdu translator:\\ \hline
%\small{According to the territory's people the pamphlets from the Taaliban had been read in the announcements in all the mosques of the Northern Wazeerastan.}\\\hline
%English post-editor:\\ \hline
%\small{According to locals, the pamphlet released by the Taliban was read out on the loudspeakers of all the mosques in North Waziristan.}\\\hline
%LDC professional:\\ \hline
%\small{According to the local people, the Taliban's pamphlet was read over the loudspeakers of all mosques in North Waziristan.} \\\hline
%\end{tabular}
%\caption{Different versions of translations.}
%\label{translations}
%\end{table}

There might be a potential concern that the competent translators are native Urdu speakers, who can understand the source sentences well but might not be able to express the original meanings in English as natural as in Urdu. It is feasible to include native English speakers to post-edit the translated English sentences into more adequate, standard and error-free ones, i.e., professional English. We aim to show that the collaboration design of two heads, non-professional Urdu speakers and non-professional English speakers, would yield better outputs than either one working in isolation, and can better approximate the results from professional trained translators. To this end, in addition to collecting multi-version of translations per source sentence, we also post another MTurk task where we asked US-based Turkers (who are likely native English speakers) to edit the translations into more fluent and grammatical sentences as the post-edited versions of the original translations. Table 1 gives an example of the whole process of translation: the unedited translations that we collected in the translation pass typically contain many simple mistakes like mis-spellings, typos, and awkward word choices. The translations often reflect non-native English, but are generally done conscientiously (in spite of the relatively small payment).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% table goes HERE!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We collect 3 different translations per Urdu sentence, and have each translation edited by 3 editors. This gives us a total of 12 sentences (3 unedited, 9 edited) per original Urdu sentence.  For comparison, we have 4 professional translations for each source sentence.

\subsection{Analysis}

We know from inspection that translations seem to improve with editing (Table \ref{translations}). Given the data from MTurk, we explore whether this is the case in general: Do all translations improve with editing? To what extent does the individual translator and the individual editor effect the quality of the final sentence?

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{translation-editor-scatter}
  \caption{Relationship between editor aggressiveness and effectiveness. Each point represents an editor/translation pair. Aggressiveness (x-axis) is measured as the TER between the pre-edit and post-edit version of the translation, and effectiveness (y-axis) is measured as the average amount by which the editing reduces the translation's TER$_{gold}$. While many editors make only a few changes, those who make many changes can bring the translation substantially closer to professional quality.}
    \label{editor-scatter}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{editor-bins}
  \caption{Effect of editing on translations of varying quality. Rows reflect bins of editors, with the worse editors (those whose changes result in increased TER$_{gold}$) on the top and the most effective editors (those whose changes result in the largest reduction in TER$_{gold}$) on the bottom. Bars reflect bins of translations, with the highest TER$_{gold}$ translations on the left, and the lowest on the right. We can see from the consistently negative $\Delta$ TER$_{gold}$ in the bottom row that good editors are able to improve both good and bad translations.}
    \label{editor-bins}
\end{figure}

We use \textit{translation edit rate} (TER) as a measure of translation similarity. TER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar. To capture the quality (``professionalness'') of a translation, we take the average TER of the translation against each of our gold translations. That is, we define TER$_{gold}$ of translation $t$ as

\begin{align}
TER_{gold} = \frac{1}{4}\sum\limits_{i = 1}^4 TER(gold_i, t)
\end{align}

where a lower TER$_{gold}$ is indicative of a higher quality (more professional-sounding) translation.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=\linewidth]{ter-rerank}
  \caption{Three alternative translations (left) and the edited versions of each (right). Each edit on the right was produced by a different editor. Order reflects the TER$_{gold}$ of each translation, with the lowest TER$_{gold}$ on the top. Some translators receive low TER$_{gold}$ scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor.}
    \label{rerank}
\end{figure*}

We first look at editors along two dimensions: their aggressiveness and their effectiveness. Some editors may be very aggressive (they make many changes to the original translation) but still be ineffective (they fail to bring the quality of the translation closer to that of a professional). We measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor's translations; higher TER implies more aggressive editing. To measure effectiveness, we look at the change in TER$_{gold}$ that results from the editing; negative $\Delta$TER$_{gold}$ means the editor effectively improved the quality of the translation, while positive $\Delta$TER$_{gold}$ means the editing actually brought the translation further from our gold standard.


Figure \ref{editor-scatter} shows the relationship between these two qualities for individual editor/translation pairs. We see that while most translations require only a few edits, there are a large number of translations which improve substantially after heavy editing. This trend conforms to our intuition that editing is most useful when the translation has much room for improvement, and opens the question of whether good editors can offer improvements to translations of all qualities.





To address this question, we split our translations into 5 bins, based on their TER$_{gold}$.  We also split our editors into 5 bins, based on their effectiveness (i.e. the average amount by which their editing reduces TER$_{gold}$). Figure \ref{editor-bins} shows the degree to which editors at each level are able to improve the translations from each bin. We see that good editors are able to make improvements to translations of all qualities, but that good editing has the greatest impact on lower quality translations. This result suggests that finding good editor/translator pairs, rather than good editors and good translators in isolation, should produce the best translations overall. Figure \ref{rerank} gives an example of how an initially medium-quality translation, when combined with good editing, produces a better result than the higher-quality translation paired with mediocre editing.


%Figure 1 gives more typical translation examples.

\section{Problem Formulation}
The problem definition of the crowdsourcing translation task is quite obvious: given the source sentences to translate and the Turkers (i.e., translators and post-editors) on MTurk, we choose the best translated and post-edited HIT as output.

Our method operates over a heterogeneous network that includes translators, post-editors and translated sentences. We frame both components of HIT and Turkers into graphs, using relationships (i.e., semantic similarity, Turker collaboration and authorship correspondingly) to connect these parts as a co-ranking paradigm \cite{a28,a30}. Let $G$ denote the heterogeneous graph with nodes $V$ and edges $E$, and $G$ = ($V$,$E$) = ($V_T, V_H, E_T, E_H, E_{TH})$. $G$ is divided into three subgraphs, $G_T$, $G_H$, and $G_{TH}$. $G_H$ = ($V_H,E_H$) is a weighted undirected graph representing the HIT and their relationships. Let $V_H$ = $\{h_i|h_i \in V_H\}$ denote a collection of $|V_H|$ translated and edited sentences, and $E_H$ the set of linkage representing affinity between them, established by textual similarity between the translated sentences (see Section 3.4 for details). $G_T$ = ($V_T,E_T$) is a weighted undirected graph representing the collaborative ties among Turkers. $V_T$ = $\{t_i|t_i \in V_T\}$ is the set of working pairs with size $|V_T|$. Links $E_T$ among Turkers are established by their shared \textit{translation} and \textit{post-editing} collaborations. Each collaboration would produce an output HIT. $G_{TH} = (V_{TH},E_{TH})$ is an unweighted bipartite graph that ties $G_T$ and $G_H$ together and represents ``authorship''. The graph $G$ consists of nodes $V_{TH}$ = $V_T \cup V_H$ and edges $E_{TH}$ connecting each HIT with its generators. Typically, an HIT is generated by the collaboration of a translator and a post-editor. The three sub-networks are illustrated in Figure \ref{frame}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.68\textwidth]{framework}
  \caption{2-step collaborative crowdsourcing translation model based on graph ranking framework including three sub-networks. The undirected links between users denotes \textit{translation-editing} collaboration. The undirected links between HIT indicate semantic affinity. A bipartite graph (whose edges are shown with the larger arrows) ties HIT and Turker networks together by authorship. A dashed circle indicates the group of candidates for a source sentence with different weighting scores.}
  \label{frame}
\end{figure*}

\subsection{Inter-Graph Ranking}

The framework includes three random walks, one on $G_T$, one on $G_H$ and the other one on $G_{TH}$. A random walk on a graph is a Markov chain, its states being the vertices of the graph. It can be described by a square matrix, where the dimension is the number of vertices in the graph. The stochastic matrix prescribing the transition probabilities from one vertex to the next. The mutual reinforcement framework couples the two random walks on $G_T$ and $G_H$ that rank HIT and Turkers in isolation. The ranking method allows to obtain a more global ranking by taking into account the intra-/inter-component dependencies. In the following sections we first describe how we obtain the rankings on $G_T$ and $G_H$, and then move on to discuss how the two are coupled.

The ranking chain shown in Figure \ref{frame} captures the following intuitions behind. An HIT is important if 1) it is correlated with many of the other generated HIT; 2) it is authored by better qualified translators and/or post-editors. Analogously, a Turker pair is believed to be better qualified if 1) the editor is collaborating with a good translator and vice versa; 2) the working pair has authored highly voted HIT. This ranking schema is actually a reinforced process across the heterogeneous graphs. We use two vectors $\textbf{h}=[\pi(h)]_{|h| \times 1}$ and $\textbf{t}=[\pi(t)]_{|t| \times 1}$ to denote the saliency scores $\pi(.)$ of HIT and Turkers. The mentioned two intuitions could be formulated as follows:

$\bullet$ \textbf{Homogeneity.} We use an adjacency matrix $[M]_{|\textbf{h}|\times |\textbf{h}|}$ to describe the homogeneous affinity between HIT, and use $[N]_{|\textbf{t}| \times |\textbf{t}|}$ between Turkers.
\begin{equation}
\textbf{h} \propto {M}^{T} \textbf{h}, \qquad \textbf{t} \propto {N}^{T} \textbf{t}
\end{equation}

where $h=|V_H|$ is the number of vertices in the HIT graph and $t=|V_T|$ is the number of vertices in the Turker graph. The adjacency matrix $[M]$ denotes the transition probabilities among HIT, and analogously matrix $[N]$ to describe the affinity between Turker collaboration pairs.

$\bullet$ \textbf{Heterogeneity.} We use an adjacency matrix $[\hat{W}]_{|\textbf{h}| \times |\textbf{t}|}$ and $[\bar{W}]_{|\textbf{t}|\times |\textbf{h}|}$ to describe the authorship between the output HIT and the producer Turker pairs from both of the HIT-to-Turker and Turker-to-HIT perspectives.
\begin{equation}
\textbf{h} \propto \hat{W}^{T} \textbf{t}, \qquad \textbf{t} \propto \bar{W}^{T} \textbf{h}
\end{equation}

All affinity matrices will be defined in the next section. By fusing the above equations, we can have the following iterative calculation in matrix forms. For numerical computation of the saliency scores, the initial scores of all sentences and images are set to 1 and the following two steps are alternated until convergence to select the best HIT. The ranking is formulated as:

\textbf{Step 1}: compute the saliency scores of HIT, and then normalize using $\ell$-1 norm.
\begin{equation}
\begin{aligned}
\textbf{h}^{(n)}&=(1-\lambda) {M}^{T}\textbf{h}^{(n-1)}+\lambda \hat{W}\textbf{t}^{(n-1)}\\
\textbf{h}^{(n)}&=\textbf{h}^{(n)}/||\textbf{h}^{(n)}||_1
\end{aligned}
\end{equation}

\textbf{Step 2}: compute the saliency scores of Turker pairs, and then normalize in $\ell$-1 norm.
\begin{equation}
\begin{aligned}
\textbf{t}^{(n)}&=(1-\lambda) {N}^{T}\textbf{t}^{(n-1)}+\lambda \bar{W}\textbf{h}^{(n-1)}\\
\textbf{t}^{(n)}&=\textbf{t}^{(n)}/||\textbf{t}^{(n)}||_1
\end{aligned}
\end{equation} where $\lambda$ specify the relative contributions to the saliency scores trade-off from the homogeneous affinity and the heterogeneous affinity. In order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible. To this end, we must make the \textbf{h} and \textbf{t} \textit{column stochastic} to force transition matrix stochastic \cite{a20}. \textbf{h} and \textbf{t} are therefore normalized after each iteration in Equation (3) and (4).

\subsection{Intra-Graph Ranking}
The standard PageRank algorithm starts from any node, then randomly selects a link from that node to follow considering the weighted transition matrix, or jumps to a random node with equal probability. Note that the generated HIT could be pre-judged to be of different linguistics qualities \cite{a14,zaidan-callisonburch:2011:ACL-HLT2011a}, we can incorporate textual quality as the transitional prior. However, since linguistics quality is not the focus of this work, we simplify the study by starting with the standard ranking. %the general textual quality judgement described in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. We use the following criteria as indicators of the quality of the generations:
%
%\textbf{Sentence-Level}. The first set of features attempt to discriminate good English sentences from bad ones before the graph ranking process.
%
%$\bullet$ Language model features: each sentence is assigned with a log probability and per-word perplexity score, using a 5-gram language model trained on the English Gigaword corpus.
%
%$\bullet$ Sentence length features: a good translation tends to be comparable in length to the source sentence, whereas an overly short or long translation is probably bad. We add two features that are the ratios of the two lengths (one penalizes short sentences and one penalizes long ones).
%
%$\bullet$ Web n-gram match percentage: we assign a score to each sentence based on the percentage of the n-grams (up to length 5) in the translation that exist in the Google N-Gram Database.
%
%$\bullet$ Web n-gram geometric average: we calculate the average over the different n-gram match percentages (similar to the way BLEU is computed). We add three features corresponding to max n-gram lengths of 3, 4, and 5.
%
%\textbf{Turker-Level.} We add Turker-level features that evaluate a translation based on the generators who provided the translation and post-editing.
%
%$\bullet$ Aggregate features: for each sentence-level feature above, we have a corresponding feature computed over all of that worker's translations. The score for the working pair is calculated as the 1) average, 2) maximum and 3) minimum score of both Turkers.
%
%$\bullet$ Activity features: we also investigate the total number of productions and the ratio of the high quality productions from the working pair.
%
%We could also use the features such as location information or language ability information. Since currently these information is not well certified, we temporarily leave it out of the Turker-Level feature now.
%
%After calculation of all features, we sort out an overall score to measure the linguistics and Turker quality based on regression when apply the linear combination measurement in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}, the quality scores are denoted as $\textbf{h}_0$ for HIT and $\textbf{t}_0$ for Turker pairs.

In a simple random walk, it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts. Then \textbf{h} and \textbf{t} should be calculated as:
\begin{equation}
\textbf{h}=\mu M^{T}\textbf{h} + (1-\mu) \frac{\textbf{1}}{|V_H|}
\end{equation}

and
\begin{equation}
\textbf{t}=\mu N^{T}\textbf{t} + (1-\mu) \frac{\textbf{1}}{|V_T|}
\end{equation} where \textbf{1} is a vector with all elements equaling to 1 and the size is correspondent to the size of $V_H$ or $V_T$. $\mu$ is the damping factor usually set to 0.85, as in the PageRank algorithm.

\subsection{Affinity Matrix Establishment}
We introduce the affinity matrix calculation, including homogeneous affinity (i.e., ${M},{N}$) and heterogeneous affinity (i.e., $\hat{W},\bar{W}$).

The HIT collection can be modeled as a weighted undirected graph. Nodes in the graph represent sentences, edges represent inter-sentential relatedness, and their weights are computed via cosine similarity. The adjacency matrix M describes such a graph with each entry corresponding to the weight of an edge.
\begin{equation}
\begin{aligned}
\mathcal{F}&(h_i,h_j)=\frac{h_i \cdot h_j}{||h_i||||h_j||}\\
&M_{ij}=\frac{\mathcal{F}(h_i,h_j)}{\sum_k \mathcal{F}(h_i,h_k)}
\end{aligned}
\end{equation} where $\mathcal{F}(.)$ is the cosine similarity and $h$ is a term vector corresponding to an HIT. We treat an HIT as a short document and weight each term with \textit{tf.idf} \cite{a16}, where \textit{tf} is the term frequency and \textit{idf} is the inverse document frequency.

The Turker graph is an undirected graph based on the collaboration linkage. When $t_i$ and $t_j$ have a shared ``collaboration'', we add an edge between $t_i$ and $t_j$. There are 3 different schema to define collaborations, e.g., pairs with the same translator and/or post-editor, which will be discussed in details in the experiment section. Let the function $\mathcal{I}(t_i,t_j)$ denote the times of shared ``collaborations'' (\#c) when there is an edge between $t_i$ and $t_j$. The adjacency matrix N is then defined as:
\begin{equation}
\begin{aligned}
\mathcal{I}(t_i,t_j)&=
\begin{cases}
\#c &(e_{ij} \in E_T) \\
0 &\text{otherwise}
\end{cases}, \\
N_{ij}&=\frac{\mathcal{I}(t_i,t_j)}{\sum_k \mathcal{I}(t_i,t_k)}
\end{aligned}
\end{equation}

In the bipartite HIT-Turker graph $G_{TH}$, the entry $E_{TH}(i, j)$ is an indicator function denoting whether the HIT $h_i$ is generated by $t_j$:
\begin{equation}
\mathcal{A}(h_i,t_j)=
\begin{cases}
1 &(e_{ij} \in E_{TH}) \\
0 &\text{otherwise}
\end{cases}
\end{equation}

Through $E_{TH}$ we define the weight matrices $\bar{W}_{ij}$ and $\hat{W}_{ij}$, containing the conditional probabilities of transitions from $h_i$ to $t_j$ and vice versa:
\begin{equation}
\begin{aligned}
&\bar{W}_{ij}=\frac{\mathcal{A}(h_i,t_j)}{\sum_k \mathcal{A}(h_i,t_k)},\\
&\hat{W}_{ij}=\frac{\mathcal{A}(h_i,t_j)}{\sum_k \mathcal{A}(h_k,t_j)}
\end{aligned}
\end{equation}

\section{Experiments and Evaluation}
\subsection{Data}
We translated the Urdu side of the Urdu-English test set of the 2009 NIST MT Evaluation Workshop, used in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. The set consists of 1,792 Urdu sentences from a variety of news and online sources. The set includes four different reference translations for each source sentence, produced by professional translation agencies. NIST contracted the LDC to oversee the translation process and perform quality control.

This particular dataset, with its multiple reference translations, is very useful because we can measure the quality range for professional translators, which gives us an idea of whether the crowdsourced translations could better approach to the quality of a professional translator.

52 different Turkers took part in the translation task, each translating 138 sentences on average. In the editing task, 320 Turkers participated, averaging 56 sentences each.

\subsection{Evaluation}
To measure the quality of the translations, we make use of the existing professional translations. Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score \cite{a17} for one professional translator P1 using the other three P2,3,4 as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. We can examine the results from different translation methods compares to this range by calculating the BLEU scores against the same four sets of three reference translations. We will evaluate different strategies for selecting different translation sets, and see how much each improves on the BLEU score, compared to randomly picking from among the Turker translations.

%We also evaluate translation quality by using reference sets to score various submissions to the NIST MT evaluation. Specifically, we measure the correlation (using Pearson r) between BLEU scores of MT systems measured against non-professional translations, and BLEU scores measured against professional translations. Since the main purpose of the NIST dataset was to compare MT systems against each other, this is a more direct fitness-for-task measure. We chose the middle 6 systems (in terms of performance) submitted to the NIST evaluation, out of 12, as those systems were fairly close to each other, with less than 2 BLEU points separating them.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{result}
  \caption{Overall BLEU performance for all methods. The highlighted yellow bars indicate methods based on both HIT and Turker information, with and without post-editing. We directly report the results of the Linear Feature Crowdsourcing in this figure.}
\end{figure*}

\subsection{Comparison Methods}
We first include an intuitive method of random selection, picking HIT out of all generations at random, which could be estimated as a lower bound. As mentioned, we evaluate the reference sets against each other, in order to quantify the concept of ``professional quality''. We establish the performance of professional translators, calculate the upper bounds for Turkers' translation quality, and then carry out a set of experiments that demonstrate the effectiveness of our model. Each number reported in the experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.

For the second group, we perform four oracle experiments to determine if there exist high-quality Turker translations in the first place. The first oracle operates on the segment level on the sentences produced by translators only: for each source segment, choose from the translations the one that scores highest against the reference sentence. The second oracle is applied on the segment level as well but on the productions from the collaboration of translators and post-editors. The third oracle operates on the worker level: for each source segment, choose from the translations the one provided by the worker whose translations (over all sentences) score the highest. Analogously, the fourth oracle is selected from the sentences from the collaboration of translators and post-editors, based on the worker level. Note that oracle methods are the most ideal possible solutions under our scenario.

We also examine other two voting-inspired methods, since the majority vote usually works well in real world NLP problems. The first selects the translation with the minimum average TER \cite{a18} against the other translations, since that would be a ``consensus'' translation. The second method selects the translation generated by the Turkers who provide translations with the minimum average TER.

The last group of methods are based on the utilization of both HIT and Turker information, either based on linear combination or on the graph based random walking. We directly report the results from the previous crowdsourcing translation system using raw translations and edited translation with all kinds of additional features \cite{zaidan-callisonburch:2011:ACL-HLT2011a}\footnote{Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data. We do not re-implement this baseline but report the results from the paper directly. According to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values.}. Also, we include our proposed collaborative crowdsourcing translation method, based on the co-ranking of HIT and Turker collaboration pairs, using 1) unedited translations only and 2) the edited sentences after translator-editor collaborations. For a full comparison, we also include variations of different ranking modelings based on raw translations and edited translations in further discussions.

\subsection{Results and Analysis}

We establish the performance of professional translators, calculate oracle upper bounds on Turker translation quality, and carry out a set of experiments that demonstrate the effectiveness of our model compared with other baselines. Each number reported in this section is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.

As expected, random selection yields bad performance with a BLEU score of 30.52.

The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations. On average, evaluating one reference set against the other three gives a BLEU score of 42.38. To make the gap clearer, the output of a state-of-the-art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. The LowestTER selects the translation with the minimum average TER \cite{a18} against the other three translations, since that would be a ``consensus'' translation. This approach achieves BLEU scores of 35.78. %The second method selects the translation from the Turkers with the minimum average TER based on all their translations. These approaches achieve BLEU scores of 34.41 and 33.28, respectively.

A raw set of translations without post-editing scores 28.13 on average based on linear-based combination and 38.89 from on the graph-based ranking, which highlights the loss in quality when collecting translations from amateurs. The linear combination of all features for the crowdsourcing translation achieves a score of 39.06. While the structure information is incorporated into the graph-based ranking framework, the performance achieves a score of 41.43, which verifies the hidden collaboration networks between HIT and Turkers are indeed useful.

Besides, we evaluated the selection methods by measuring correlation with the references, in terms of BLEU scores assigned to outputs of MT systems. The results, in Table 2, tell a fairly similar story as evaluating with BLEU: references and oracles naturally perform very well, and the loss in quality when selecting arbitrary Turker translations is largely eliminated using our selection strategy.




\subsection{Analysis}
\subsubsection{Cost Reduction}
The most prominent advantage of crowdsoucing translation would be the low cost to spend. We paid a reward of \$0.10 to translate a sentence, \$0.25 to edit a set of ten sentences. Therefore, we had the following costs:

$\bullet$ Translation cost: \$716.80

$\bullet$ Editing cost: \$447.50

(If not done redundantly, those values would be \$179.20, \$44.75, respectively.)
Adding Amazon's 10\% fee, this brings the grand total to under \$1,500, spent to collect 7,000+ translations, 17,000+ edited translations. While the combined cost of our data collection effort is quite low considering the amount of collected data, it is more attractive when the cost could be reduced further without losing much in translation quality by finding more professional non-experts in the Turker graph, and decreasing the amount of translated/edited translations. We indeed improved the performance over the linear combination (regression) based method and reduce costs by ranking Turker graph and HIT graph correspondingly.



\subsubsection{Parameter Tuning}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{parameters}
  \caption{Effect of HIT-Turker coupling by $\lambda$.}
\end{figure}

The professional translations are used in our approach for measuring the performance against the ground truth and for tuning the weights of the parameters.

There are two parameters in our experimental setups: $\mu$ controls the probability to start a new random walk and $\lambda$ deals with coupling between the HIT and Turker sub-graphs. We set the damping factor $\mu$ to 0.85 following the standard PageRank paradigm. We opted for more or less generic parameter values as we did not want to tune our framework to the specific dataset at hand. We examined
the parameter $\lambda$ which controls the balance of the HIT-Turker graph in more detail. We experimented with values ranging from 0 to 1, with a step size of 0.05. Small $\lambda$ values place little emphasis on the coupling part, whereas larger values rely more heavily
on the co-ranking. Overall, we observed better performance with values within the range of 0.05-0.15. This suggests that both sources of information for HIT and their authors are important for the crowdsourcing translation task. All our experiments used the same $\lambda$ value which was set to 0.1.

\subsubsection{Component Strategy}
We next examine the relative contribution of different strategies when modeling our proposed ranking framework. Specifically, we first examine the centroid based ranking on the HITs sub-graph to see the effect of voting among translated sentences, denoted as \textit{plain ranking}. Then we incorporate the standard random walk the Turker graph to make utilization of the structural information but without any collaboration included, i.e., on the sentence graph only while no edges between Turkers in the Turker graph is included. The co-ranking paradigm is exactly the same with the framework mentioned in Section 3.2 with simplified structures.

In the following steps, we examine the two-step collaboration based HIT-Turker graph with several variations of edge establishment: the nodes are the translator/post-editor working pairs. The edges are connected when 1) two nodes share a translator only, 2) two nodes share a post-editor only, and 3) two nodes share either a translator or a post-editor. %For all of the ranking paradigm, we include the comparison of two versions with and without linguistics quality prior.

\begin{table}[t]
\centering
\begin{tabular}{|c||c|} \hline
Plain ranking   &38.89        \\   \hline \hline
w/o collaboration &38.88  	           \\ \hline \hline
Shared translator  &41.38	          \\  \hline
Shared post-editor    &40.04 \\ \hline
Shared Turker   &\textbf{41.43}  \\  \hline
\end{tabular}
\caption{Variations of all component settings.}
\end{table}

An interesting observation is that when modeling the linkage between the collaboration pairs, we note that to establish linkage when the Turker pairs share either translator or the post-editor will achieve the best performance, while to establish linkage when they share only post-editors or translators will lead to worse results. It is intuitive to understand that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence in return get a good selection.

%\section{Related Work}
%Extraneous data source could always be a supplement to improve MT models so that they are better suited to the low resource setting \cite{a21,a22}. Transfer learning based models, or designing models that are capable of learning translations from monolingual corpora \cite{a23,a24}.
%
%Dawid and Skene \shortcite{a1} investigated filtering annotations using the EM algorithm, estimating annotator-specific error rates in the context of patient medical records. Snow et al. \shortcite{a2} were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense disambiguation. Their approach, based on majority voting, had a component for annotator bias correction. They showed that for such tasks, a few non-expert labels usually suffice.
%
%Whitehill et al. \shortcite{a3} proposed a probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system generatively models image difficulty, as well as noisy, even adversarial, annotators. They apply their method to simulated labels rather than real-life labels.
%
%Callison-Burch \shortcite{a4} proposed several ways to evaluate MT output on MTurk. One such method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch \shortcite{a5} solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems.
%
%That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze \shortcite{a6}, focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel \shortcite{a7}, focusing on the design of the translation HIT, and by Irvine and Klementiev \shortcite{a8}, who created translation lexicons between English and 42 rare languages.
%
%Resnik et al. \shortcite{a9} explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion. The paraphrased source would then be retranslated to produce a different translation, hopefully more coherent than the original.

\section{Conclusion}
We have proposed a two-step non-professional collaboration based co-ranking model to select the best crowdsourced translation on the heterogeneous HIT-Turker graph, and we have demonstrated that compared with a series of MT methods, it is possible to obtain improved performance near professional quality and even less costs from non-professional collaborations.

We believe that crowdsourcing can play a pivotal role in future efforts to create parallel translation datasets. Beyond the cost and scalability, crowdsourcing provides access to languages that currently fall outside the scope of statistical machine translation research. There are possible ways to further improve the crowdsourcing translation and reduce costs by: 1) build ground truth against good Turkers only, instead of professionals, once they have been identified and 2) predict when it is unnecessary to solicit more translations after a certain threshold.

\section*{Acknowledgements}
This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled ``Crowdsourcing Translation'' (contract D12PC00368). The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements by DARPA or the U.S. Government. This research was supported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft, Google and Facebook.

\bibliographystyle{acl2014}
\bibliography{acltranslation}




%The acknowledgments should go immediately before the references.  Do
%not number the acknowledgments section. Do not include this section
%when submitting your paper for review.





\end{document}
