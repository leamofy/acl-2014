\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{helvet}
\usepackage{courier}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{relsize}
\usepackage{hhline}
\usepackage[usenames]{color}
\usepackage{colortbl,booktabs}
%\usepackage{slashbox}
\usepackage{balance}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{url}
\usepackage{algpseudocode}
\definecolor{mygray}{gray}{.5}

\renewcommand{\baselinestretch}{0.95}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


% Alternate titles.

\title{Are Two Heads Better than One? Crowdsourced Translation via a \\Two-Step Collaboration of Non-Professional Translators and Editors}

\author{Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch \\
Computer and Information Science Department, \\
University of Pennsylvania, Philadelphia, PA, U.S.A.\\
\normalsize{{\tt \{ruiyan,gmingkun,epavlick\}@seas.upenn.edu, ccb@cis.upenn.edu}}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Crowdsourcing is a viable mechanism for creating training data for machine translation.  It provides a low cost, fast turn-around way of processing large volumes of data. However, when compared professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is a necessity for crowdsourcing to work well. In this paper, we use a two-step collaboration process with translation and post-editing by non-professionals.  We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals. With appropriate quality control, we are able to distinguish acceptable translations from bad ones.  We demonstrate our model on the  Mechanical Turk translations of the NIST 2009 Urdu-to-English evaluation set, and show that our models are able to achieve better quality than previously published methods.
\end{abstract}




%Figure 1 gives more typical translation examples.

\section{Problem Formulation}
The problem definition of the crowdsourcing translation task is quite obvious: given the source sentences to translate and the Turkers (i.e., translators and post-editors) on MTurk, we choose the best translated and post-edited HIT as output.

Our method operates over a heterogeneous network that includes translators, post-editors and translated sentences. We frame both components of HIT and Turkers into graphs, using relationships (i.e., semantic similarity, Turker collaboration and authorship correspondingly) to connect these parts as a co-ranking paradigm \cite{a28,a30}. Let $G$ denote the heterogeneous graph with nodes $V$ and edges $E$, and $G$ = ($V$,$E$) = ($V_T, V_H, E_T, E_H, E_{TH})$. $G$ is divided into three subgraphs, $G_T$, $G_H$, and $G_{TH}$. $G_H$ = ($V_H,E_H$) is a weighted undirected graph representing the HIT and their relationships. Let $V_H$ = $\{h_i|h_i \in V_H\}$ denote a collection of $|V_H|$ translated and edited sentences, and $E_H$ the set of linkage representing affinity between them, established by textual similarity between the translated sentences (see Section 3.4 for details). $G_T$ = ($V_T,E_T$) is a weighted undirected graph representing the collaborative ties among Turkers. $V_T$ = $\{t_i|t_i \in V_T\}$ is the set of working pairs with size $|V_T|$. Links $E_T$ among Turkers are established by their shared \textit{translation} and \textit{post-editing} collaborations. Each collaboration would produce an output HIT. $G_{TH} = (V_{TH},E_{TH})$ is an unweighted bipartite graph that ties $G_T$ and $G_H$ together and represents ``authorship''. The graph $G$ consists of nodes $V_{TH}$ = $V_T \cup V_H$ and edges $E_{TH}$ connecting each HIT with its generators. Typically, an HIT is generated by the collaboration of a translator and a post-editor. The three sub-networks are illustrated in Figure \ref{frame}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.68\textwidth]{framework}
  \caption{2-step collaborative crowdsourcing translation model based on graph ranking framework including three sub-networks. The undirected links between users denotes \textit{translation-editing} collaboration. The undirected links between candidate translations indicate semantic similarity among HITs. A bipartite graph ties HIT and Turker networks together by authorship (to make the figure clearer, some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source sentence to translate.}
  \label{frame}
\end{figure*}

\subsection{Inter-Graph Ranking}

The framework includes three random walks, one on $G_T$, one on $G_H$ and the other one on $G_{TH}$. A random walk on a graph is a Markov chain, its states being the vertices of the graph. It can be described by a square matrix, where the dimension is the number of vertices in the graph. The stochastic matrix prescribing the transition probabilities from one vertex to the next. The mutual reinforcement framework couples the two random walks on $G_T$ and $G_H$ that rank HIT and Turkers in isolation. The ranking method allows to obtain a more global ranking by taking into account the intra-/inter-component dependencies. In the following sections we first describe how we obtain the rankings on $G_T$ and $G_H$, and then move on to discuss how the two are coupled.

The ranking chain shown in Figure \ref{frame} captures the following intuitions behind. An HIT is important if 1) it is correlated with many of the other generated HIT; 2) it is authored by better qualified translators and/or post-editors. Analogously, a Turker pair is believed to be better qualified if 1) the editor is collaborating with a good translator and vice versa; 2) the working pair has authored highly voted HIT. This ranking schema is actually a reinforced process across the heterogeneous graphs. We use two vectors $\textbf{h}=[\pi(h)]_{|h| \times 1}$ and $\textbf{t}=[\pi(t)]_{|t| \times 1}$ to denote the saliency scores $\pi(.)$ of HIT and Turkers. The mentioned two intuitions could be formulated as follows:

$\bullet$ \textbf{Homogeneity.} We use an adjacency matrix $[M]_{|\textbf{h}|\times |\textbf{h}|}$ to describe the homogeneous affinity between HIT, and use $[N]_{|\textbf{t}| \times |\textbf{t}|}$ between Turkers.
\begin{equation}
\textbf{h} \propto {M}^{T} \textbf{h}, \qquad \textbf{t} \propto {N}^{T} \textbf{t}
\end{equation}

where $h=|V_H|$ is the number of vertices in the HIT graph and $t=|V_T|$ is the number of vertices in the Turker graph. The adjacency matrix $[M]$ denotes the transition probabilities among HIT, and analogously matrix $[N]$ to describe the affinity between Turker collaboration pairs.

$\bullet$ \textbf{Heterogeneity.} We use an adjacency matrix $[\hat{W}]_{|\textbf{h}| \times |\textbf{t}|}$ and $[\bar{W}]_{|\textbf{t}|\times |\textbf{h}|}$ to describe the authorship between the output HIT and the producer Turker pairs from both of the HIT-to-Turker and Turker-to-HIT perspectives.
\begin{equation}
\textbf{h} \propto \hat{W}^{T} \textbf{t}, \qquad \textbf{t} \propto \bar{W}^{T} \textbf{h}
\end{equation}

All affinity matrices will be defined in the next section. By fusing the above equations, we can have the following iterative calculation in matrix forms. For numerical computation of the saliency scores, the initial scores of all sentences and images are set to 1 and the following two steps are alternated until convergence to select the best HIT. The ranking is formulated as:

\textbf{Step 1}: compute the saliency scores of HIT, and then normalize using $\ell$-1 norm.
\begin{equation}
\begin{aligned}
\textbf{h}^{(n)}&=(1-\lambda) {M}^{T}\textbf{h}^{(n-1)}+\lambda \hat{W}\textbf{t}^{(n-1)}\\
\textbf{h}^{(n)}&=\textbf{h}^{(n)}/||\textbf{h}^{(n)}||_1
\end{aligned}
\end{equation}

\textbf{Step 2}: compute the saliency scores of Turker pairs, and then normalize in $\ell$-1 norm.
\begin{equation}
\begin{aligned}
\textbf{t}^{(n)}&=(1-\lambda) {N}^{T}\textbf{t}^{(n-1)}+\lambda \bar{W}\textbf{h}^{(n-1)}\\
\textbf{t}^{(n)}&=\textbf{t}^{(n)}/||\textbf{t}^{(n)}||_1
\end{aligned}
\end{equation} where $\lambda$ specify the relative contributions to the saliency scores trade-off from the homogeneous affinity and the heterogeneous affinity. In order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible. To this end, we must make the \textbf{h} and \textbf{t} \textit{column stochastic} to force transition matrix stochastic \cite{a20}. \textbf{h} and \textbf{t} are therefore normalized after each iteration in Equation (3) and (4).

\subsection{Intra-Graph Ranking}
The standard PageRank algorithm starts from any node, then randomly selects a link from that node to follow considering the weighted transition matrix, or jumps to a random node with equal probability. Note that the generated HIT could be pre-judged to be of different linguistics qualities \cite{a14,zaidan-callisonburch:2011:ACL-HLT2011a}, we can incorporate textual quality as the transitional prior. However, since linguistics quality is not the focus of this work, we simplify the study by starting with the standard ranking. %the general textual quality judgement described in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. We use the following criteria as indicators of the quality of the generations:
%
%\textbf{Sentence-Level}. The first set of features attempt to discriminate good English sentences from bad ones before the graph ranking process.
%
%$\bullet$ Language model features: each sentence is assigned with a log probability and per-word perplexity score, using a 5-gram language model trained on the English Gigaword corpus.
%
%$\bullet$ Sentence length features: a good translation tends to be comparable in length to the source sentence, whereas an overly short or long translation is probably bad. We add two features that are the ratios of the two lengths (one penalizes short sentences and one penalizes long ones).
%
%$\bullet$ Web n-gram match percentage: we assign a score to each sentence based on the percentage of the n-grams (up to length 5) in the translation that exist in the Google N-Gram Database.
%
%$\bullet$ Web n-gram geometric average: we calculate the average over the different n-gram match percentages (similar to the way BLEU is computed). We add three features corresponding to max n-gram lengths of 3, 4, and 5.
%
%\textbf{Turker-Level.} We add Turker-level features that evaluate a translation based on the generators who provided the translation and post-editing.
%
%$\bullet$ Aggregate features: for each sentence-level feature above, we have a corresponding feature computed over all of that worker's translations. The score for the working pair is calculated as the 1) average, 2) maximum and 3) minimum score of both Turkers.
%
%$\bullet$ Activity features: we also investigate the total number of productions and the ratio of the high quality productions from the working pair.
%
%We could also use the features such as location information or language ability information. Since currently these information is not well certified, we temporarily leave it out of the Turker-Level feature now.
%
%After calculation of all features, we sort out an overall score to measure the linguistics and Turker quality based on regression when apply the linear combination measurement in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}, the quality scores are denoted as $\textbf{h}_0$ for HIT and $\textbf{t}_0$ for Turker pairs.

In a simple random walk, it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts. Then \textbf{h} and \textbf{t} should be calculated as:
\begin{equation}
\textbf{h}=\mu M^{T}\textbf{h} + (1-\mu) \frac{\textbf{1}}{|V_H|}
\end{equation}

and
\begin{equation}
\textbf{t}=\mu N^{T}\textbf{t} + (1-\mu) \frac{\textbf{1}}{|V_T|}
\end{equation} where \textbf{1} is a vector with all elements equaling to 1 and the size is correspondent to the size of $V_H$ or $V_T$. $\mu$ is the damping factor usually set to 0.85, as in the PageRank algorithm.

\subsection{Affinity Matrix Establishment}
We introduce the affinity matrix calculation, including homogeneous affinity (i.e., ${M},{N}$) and heterogeneous affinity (i.e., $\hat{W},\bar{W}$).

The HIT collection can be modeled as a weighted undirected graph. Nodes in the graph represent sentences, edges represent inter-sentential relatedness, and their weights are computed via cosine similarity. The adjacency matrix M describes such a graph with each entry corresponding to the weight of an edge.
\begin{equation}
\begin{aligned}
\mathcal{F}&(h_i,h_j)=\frac{h_i \cdot h_j}{||h_i||||h_j||}\\
&M_{ij}=\frac{\mathcal{F}(h_i,h_j)}{\sum_k \mathcal{F}(h_i,h_k)}
\end{aligned}
\end{equation} where $\mathcal{F}(.)$ is the cosine similarity and $h$ is a term vector corresponding to an HIT. We treat an HIT as a short document and weight each term with \textit{tf.idf} \cite{a16}, where \textit{tf} is the term frequency and \textit{idf} is the inverse document frequency.

The Turker graph is an undirected graph based on the collaboration linkage. When $t_i$ and $t_j$ have a shared ``collaboration'', we add an edge between $t_i$ and $t_j$. There are 3 different schema to define collaborations, e.g., pairs with the same translator and/or post-editor, which will be discussed in details in the experiment section. Let the function $\mathcal{I}(t_i,t_j)$ denote the times of shared ``collaborations'' (\#c) when there is an edge between $t_i$ and $t_j$. The adjacency matrix N is then defined as:
\begin{equation}
\begin{aligned}
\mathcal{I}(t_i,t_j)&=
\begin{cases}
\#c &(e_{ij} \in E_T) \\
0 &\text{otherwise}
\end{cases}, \\
N_{ij}&=\frac{\mathcal{I}(t_i,t_j)}{\sum_k \mathcal{I}(t_i,t_k)}
\end{aligned}
\end{equation}

In the bipartite HIT-Turker graph $G_{TH}$, the entry $E_{TH}(i, j)$ is an indicator function denoting whether the HIT $h_i$ is generated by $t_j$:
\begin{equation}
\mathcal{A}(h_i,t_j)=
\begin{cases}
1 &(e_{ij} \in E_{TH}) \\
0 &\text{otherwise}
\end{cases}
\end{equation}

Through $E_{TH}$ we define the weight matrices $\bar{W}_{ij}$ and $\hat{W}_{ij}$, containing the conditional probabilities of transitions from $h_i$ to $t_j$ and vice versa:
\begin{equation}
\begin{aligned}
&\bar{W}_{ij}=\frac{\mathcal{A}(h_i,t_j)}{\sum_k \mathcal{A}(h_i,t_k)},\\
&\hat{W}_{ij}=\frac{\mathcal{A}(h_i,t_j)}{\sum_k \mathcal{A}(h_k,t_j)}
\end{aligned}
\end{equation}

\section{Experiments and Evaluation}
\subsection{Data}
We translated the Urdu side of the Urdu-English test set of the 2009 NIST MT Evaluation Workshop, used in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. The set consists of 1,792 Urdu sentences from a variety of news and online sources. The set includes four different reference translations for each source sentence, produced by professional translation agencies. NIST contracted the LDC to oversee the translation process and perform quality control.

This particular dataset, with its multiple reference translations, is very useful because we can measure the quality range for professional translators, which gives us an idea of whether the crowdsourced translations could better approach to the quality of a professional translator.

52 different Turkers took part in the translation task, each translating 138 sentences on average. In the editing task, 320 Turkers participated, averaging 56 sentences each.

\subsection{Evaluation}
To measure the quality of the translations, we make use of the existing professional translations. Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score \cite{a17} for one professional translator P1 using the other three P2,3,4 as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. We can examine the results from different translation methods compares to this range by calculating the BLEU scores against the same four sets of three reference translations. We will evaluate different strategies for selecting different translation sets, and see how much each improves on the BLEU score, compared to randomly picking from among the Turker translations.

%We also evaluate translation quality by using reference sets to score various submissions to the NIST MT evaluation. Specifically, we measure the correlation (using Pearson r) between BLEU scores of MT systems measured against non-professional translations, and BLEU scores measured against professional translations. Since the main purpose of the NIST dataset was to compare MT systems against each other, this is a more direct fitness-for-task measure. We chose the middle 6 systems (in terms of performance) submitted to the NIST evaluation, out of 12, as those systems were fairly close to each other, with less than 2 BLEU points separating them.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{result}
  \caption{Overall BLEU performance for all methods. The highlighted yellow bars indicate methods based on both HIT and Turker information, with and without post-editing. We directly report the results of the Linear Feature Crowdsourcing in this figure.}
\end{figure*}

\subsection{Comparison Methods}
We first include an intuitive method of random selection, picking HIT out of all generations at random, which could be estimated as a lower bound. As mentioned, we evaluate the reference sets against each other, in order to quantify the concept of ``professional quality''. We establish the performance of professional translators, calculate the upper bounds for Turkers' translation quality, and then carry out a set of experiments that demonstrate the effectiveness of our model. Each number reported in the experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.

For the second group, we perform four oracle experiments to determine if there exist high-quality Turker translations in the first place. The first oracle operates on the segment level on the sentences produced by translators only: for each source segment, choose from the translations the one that scores highest against the reference sentence. The second oracle is applied on the segment level as well but on the productions from the collaboration of translators and post-editors. The third oracle operates on the worker level: for each source segment, choose from the translations the one provided by the worker whose translations (over all sentences) score the highest. Analogously, the fourth oracle is selected from the sentences from the collaboration of translators and post-editors, based on the worker level. Note that oracle methods are the most ideal possible solutions under our scenario.

We also examine other two voting-inspired methods, since the majority vote usually works well in real world NLP problems. The first selects the translation with the minimum average TER \cite{a18} against the other translations, since that would be a ``consensus'' translation. The second method selects the translation generated by the Turkers who provide translations with the minimum average TER.

The last group of methods are based on the utilization of both HIT and Turker information, either based on linear combination or on the graph based random walking. We directly report the results from the previous crowdsourcing translation system using raw translations and edited translation with all kinds of additional features \cite{zaidan-callisonburch:2011:ACL-HLT2011a}\footnote{Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data. We do not re-implement this baseline but report the results from the paper directly. According to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values.}. Also, we include our proposed collaborative crowdsourcing translation method, based on the co-ranking of HIT and Turker collaboration pairs, using 1) unedited translations only and 2) the edited sentences after translator-editor collaborations. For a full comparison, we also include variations of different ranking modelings based on raw translations and edited translations in further discussions.

\subsection{Results and Analysis}

We establish the performance of professional translators, calculate oracle upper bounds on Turker translation quality, and carry out a set of experiments that demonstrate the effectiveness of our model compared with other baselines. Each number reported in this section is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.

As expected, random selection yields bad performance with a BLEU score of 30.52.

The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations. On average, evaluating one reference set against the other three gives a BLEU score of 42.38. To make the gap clearer, the output of a state-of-the-art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in \cite{zaidan-callisonburch:2011:ACL-HLT2011a}. The LowestTER selects the translation with the minimum average TER \cite{a18} against the other three translations, since that would be a ``consensus'' translation. This approach achieves BLEU scores of 35.78. %The second method selects the translation from the Turkers with the minimum average TER based on all their translations. These approaches achieve BLEU scores of 34.41 and 33.28, respectively.

A raw set of translations without post-editing scores 28.13 on average based on linear-based combination and 38.89 from on the graph-based ranking, which highlights the loss in quality when collecting translations from amateurs. The linear combination of all features for the crowdsourcing translation achieves a score of 39.06. While the structure information is incorporated into the graph-based ranking framework, the performance achieves a score of 41.43, which verifies the hidden collaboration networks between HIT and Turkers are indeed useful.

Besides, we evaluated the selection methods by measuring correlation with the references, in terms of BLEU scores assigned to outputs of MT systems. The results, in Table 2, tell a fairly similar story as evaluating with BLEU: references and oracles naturally perform very well, and the loss in quality when selecting arbitrary Turker translations is largely eliminated using our selection strategy.




\subsection{Analysis}
\subsubsection{Cost Reduction}
The most prominent advantage of crowdsoucing translation would be the low cost to spend. We paid a reward of \$0.10 to translate a sentence, \$0.25 to edit a set of ten sentences. Therefore, we had the following costs:

$\bullet$ Translation cost: \$716.80

$\bullet$ Editing cost: \$447.50

(If not done redundantly, those values would be \$179.20, \$44.75, respectively.)
Adding Amazon's 10\% fee, this brings the grand total to under \$1,500, spent to collect 7,000+ translations, 17,000+ edited translations. While the combined cost of our data collection effort is quite low considering the amount of collected data, it is more attractive when the cost could be reduced further without losing much in translation quality by finding more professional non-experts in the Turker graph, and decreasing the amount of translated/edited translations. We indeed improved the performance over the linear combination (regression) based method and reduce costs by ranking Turker graph and HIT graph correspondingly.



\subsubsection{Parameter Tuning}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{parameters}
  \caption{Effect of HIT-Turker coupling by $\lambda$.}
\end{figure}

The professional translations are used in our approach for measuring the performance against the ground truth and for tuning the weights of the parameters.

There are two parameters in our experimental setups: $\mu$ controls the probability to start a new random walk and $\lambda$ deals with coupling between the HIT and Turker sub-graphs. We set the damping factor $\mu$ to 0.85 following the standard PageRank paradigm. We opted for more or less generic parameter values as we did not want to tune our framework to the specific dataset at hand. We examined
the parameter $\lambda$ which controls the balance of the HIT-Turker graph in more detail. We experimented with values ranging from 0 to 1, with a step size of 0.05. Small $\lambda$ values place little emphasis on the coupling part, whereas larger values rely more heavily
on the co-ranking. Overall, we observed better performance with values within the range of 0.05-0.15. This suggests that both sources of information for HIT and their authors are important for the crowdsourcing translation task. All our experiments used the same $\lambda$ value which was set to 0.1.

\subsubsection{Component Strategy}
We next examine the relative contribution of different strategies when modeling our proposed ranking framework. Specifically, we first examine the centroid based ranking on the HITs sub-graph to see the effect of voting among translated sentences, denoted as \textit{plain ranking}. Then we incorporate the standard random walk the Turker graph to make utilization of the structural information but without any collaboration included, i.e., on the sentence graph only while no edges between Turkers in the Turker graph is included. The co-ranking paradigm is exactly the same with the framework mentioned in Section 3.2 with simplified structures.

In the following steps, we examine the two-step collaboration based HIT-Turker graph with several variations of edge establishment: the nodes are the translator/post-editor working pairs. The edges are connected when 1) two nodes share a translator only, 2) two nodes share a post-editor only, and 3) two nodes share either a translator or a post-editor. %For all of the ranking paradigm, we include the comparison of two versions with and without linguistics quality prior.

\begin{table}[t]
\centering
\begin{tabular}{|c||c|} \hline
Plain ranking   &38.89        \\   \hline \hline
w/o collaboration &38.88  	           \\ \hline \hline
Shared translator  &41.38	          \\  \hline
Shared post-editor    &41.29 \\ \hline
Shared Turker   &\textbf{41.43}  \\  \hline
\end{tabular}
\caption{Variations of all component settings.}
\end{table}

An interesting observation is that when modeling the linkage between the collaboration pairs, we note that to establish linkage when the Turker pairs share either translator or the post-editor will achieve the best performance, while to establish linkage when they share only post-editors or translators will lead to worse results. It is intuitive to understand that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence in return get a good selection.

%\section{Related Work}
%Extraneous data source could always be a supplement to improve MT models so that they are better suited to the low resource setting \cite{a21,a22}. Transfer learning based models, or designing models that are capable of learning translations from monolingual corpora \cite{a23,a24}.
%
%Dawid and Skene \shortcite{a1} investigated filtering annotations using the EM algorithm, estimating annotator-specific error rates in the context of patient medical records. Snow et al. \shortcite{a2} were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense disambiguation. Their approach, based on majority voting, had a component for annotator bias correction. They showed that for such tasks, a few non-expert labels usually suffice.
%
%Whitehill et al. \shortcite{a3} proposed a probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system generatively models image difficulty, as well as noisy, even adversarial, annotators. They apply their method to simulated labels rather than real-life labels.
%
%Callison-Burch \shortcite{a4} proposed several ways to evaluate MT output on MTurk. One such method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch \shortcite{a5} solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems.
%
%That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze \shortcite{a6}, focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel \shortcite{a7}, focusing on the design of the translation HIT, and by Irvine and Klementiev \shortcite{a8}, who created translation lexicons between English and 42 rare languages.
%
%Resnik et al. \shortcite{a9} explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion. The paraphrased source would then be retranslated to produce a different translation, hopefully more coherent than the original.

\section{Conclusion}
We have proposed a two-step non-professional collaboration based co-ranking model to select the best crowdsourced translation on the heterogeneous HIT-Turker graph, and we have demonstrated that compared with a series of MT methods, it is possible to obtain improved performance near professional quality and even less costs from non-professional collaborations.

We believe that crowdsourcing can play a pivotal role in future efforts to create parallel translation datasets. Beyond the cost and scalability, crowdsourcing provides access to languages that currently fall outside the scope of statistical machine translation research. There are possible ways to further improve the crowdsourcing translation and reduce costs by: 1) build ground truth against good Turkers only, instead of professionals, once they have been identified and 2) predict when it is unnecessary to solicit more translations after a certain threshold.

\section*{Acknowledgements}
This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled ``Crowdsourcing Translation'' (contract D12PC00368). The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements by DARPA or the U.S. Government. This research was supported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft, Google and Facebook.

\bibliographystyle{acl2014}
\bibliography{acltranslation}




%The acknowledgments should go immediately before the references.  Do
%not number the acknowledgments section. Do not include this section
%when submitting your paper for review.





\end{document}
